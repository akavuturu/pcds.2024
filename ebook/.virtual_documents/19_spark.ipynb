





from pyspark import SparkContext
sc = SparkContext("local", "App Name",)

import random
num_samples = 100000000
def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)
sc.stop()











from pyspark import SparkContext
sc = SparkContext("local", "App Name",)

text_file = sc.textFile("./data/textbible/*")
counts = text_file.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile("/tmp/sparkoutput4")

sc.stop()


sc.stop()






