


import numpy as np

def line_to_triples(line: str):
    fids = np.array(line.split(), dtype=int)
    ret = []
    for i in range(1, len(fids) - 1):
        for j in  range(i + 1, len(fids)):
            source = fids[0]
            fi, fj = fids[i], fids[j]
            if source < fi:
                ret.append([fj, source, fi])
            else:
                ret.append([fj, fi, source])
            if source < fj:
                ret.append([fi, source, fj])
            else:
                ret.append([fi, fj, source])
    return ret    





line_to_triples ("1 5 8 7 9")


sc.stop()





from pyspark import SparkContext

inputdir = "../data/simple.input"
outdir = "/tmp/outputsimple.mr4"

from pyspark import SparkContext
sc = SparkContext("local", "App Name",)
rdd = sc.textFile(f"{inputdir}/*")
rdd = rdd.flatMap(line_to_triples)
### TODO add a WordCount style reducer
rdd = rdd.map(lambda x: (str(x), 1))
rdd = rdd.reduceByKey(lambda x, y: x + y)
### TODO Identify triples of count >=2
rdd = rdd.filter(lambda x: x[1]>1)
### TODO format output to just triples
rdd = rdd.map(lambda x: x[0])
rdd.saveAsTextFile(outdir)

sc.stop()             


sc.stop()











### add a parition index to each triple.
def add_index (idx, part):
    for p in part:
        yield str(p), str(idx)





def filter_diff_idx (x):
    # TODO
    pass


from pyspark import SparkContext

inputdir = "../data/simple.input"
outdir = "/tmp/output.join1"

from pyspark import SparkContext
sc = SparkContext("local", "App Name",)
rdd = sc.textFile(f"{inputdir}/*")
rdd = rdd.flatMap(line_to_triples)
rdd = rdd.mapPartitionsWithIndex(add_index)
# TODO self join
rdd = rdd.join(rdd)
# TODO filter out matches from same partition
rdd = rdd.filter(filter_diff_idx)
# TODO cleanup output
...
rdd.saveAsTextFile(outdir)

sc.stop()





sc.stop()








### Output the remaining list of friends for each friend.
def pair_lists(line: str):
    fids = np.array(line.split(), dtype=int)
    ret = []
    for i in range(1, len(fids) - 1):
        source = fids[0]
        if source < fids[i]:
            ret.append([f"{source}, {fids[i]}", np.concatenate((fids[1:i], fids[i+1:]),)])
        else:
            ret.append([f"{fids[i]}, {source}", np.concatenate((fids[1:i], fids[i+1:]),)])            
    return ret    


pair_lists ("2 1 5 8 7 9")





### Output the appropriate triples after intersection.
def output_triples(x):
    output = []
    xar = np.fromstring(x[0], dtype=int, sep=",")
    for third in x[1]:
        if xar[0] < xar[1]:
            output.append((third, xar[0], xar[1]))
        else:
            output.append((third, xar[1], xar[0]))
    return output
        


sc.stop()


from pyspark import SparkContext

inputdir = "../data/simple.input"
outdir = "/tmp/output.merge6"

from pyspark import SparkContext
sc = SparkContext("local", "App Name",)
rdd = sc.textFile(f"{inputdir}/*")
rdd = rdd.flatMap(pair_lists)
# group by and turn into lists
rdd = rdd.groupByKey().mapValues(list)
# keep only keys with two inputs
rdd = rdd.filter(lambda x: len(x[1])==2)
# TODO intersect the two lists
rdd = rdd.mapValues(lambda x: np.intersect1d(x[0], x[1]))
rdd = rdd.flatMap(output_triples)
rdd.saveAsTextFile(outdir)

sc.stop()








%%timeit -n1 -r1
from pyspark import SparkContext

inputdir = "../data/fof.input"
outdir = "/tmp/outputfof.mr"

from pyspark import SparkContext
sc = SparkContext("local", "App Name",)
rdd = sc.textFile(f"{inputdir}/*")
rdd = rdd.flatMap(line_to_triples)
...
rdd.saveAsTextFile(outdir)

sc.stop()                





%%timeit -n1 -r1
from pyspark import SparkContext

inputdir = "../data/fof.input"
outdir = "/tmp/outputfof.join"

from pyspark import SparkContext
sc = SparkContext("local", "App Name",)
rdd = sc.textFile(f"{inputdir}/*")
rdd = rdd.flatMap(line_to_triples)
...
rdd.saveAsTextFile(outdir)

sc.stop()





%%timeit -n1 -r1
from pyspark import SparkContext

inputdir = "../data/fof.input"
outdir = "/tmp/outputfof.merge"

from pyspark import SparkContext
sc = SparkContext("local", "App Name",)
rdd = sc.textFile(f"{inputdir}/*")
...
rdd.saveAsTextFile(outdir)

sc.stop()



